<!DOCTYPE html>
<html>
  <head>
    <title>Risks</title>
    <link rel="stylesheet" href="minimal-table.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
  </head>

  <body>
<div id="circle"></div>
    <!-- Site navigation menu -->
    <ul class="navbar">
      <li><a href="index.html">Home</a></li>
  <li><a href="topic.html">Topic</a></li>
	<li><a href="opportunities.html">Opportunities</a></li>
	<li><a href="risks.html">Risks</a></li>
	<li><a href="choices.html">Choices</a></li>
	<!--<li><a href="ethics.html">Ethical Reflections</a></li>-->
	<li><a href="references.html">References</a></li>
   <li><a href="process.html">Process Support</a></li>
</ul>

        <div class="content">
      <div class="group"><p>G:4</p></div>
    <!-- Main content -->
    <h1>Technology Risks</h1>
<hr>
          
<!--kevin's up and coming risks, coming to a webpage near you-->
    <p>
      One of the core issues inherent to any social media platform lies in the management
      of user-generated content. While free speech is considered a fundamental right in
      most societies, its application in digital spaces can be complex. It has become a
      tricky topic to navigate, as the line between mild-but-acceptable content and content
      that should be deleted ultimately boils down to user-tolerance. Therefore content
      moderation exists to try to filter out the egregiously harmful content from the mild.
      However, when left unchecked, content moderation processes can overreach itself to a
      point where it effectively infringes on the users' right to free speech and becomes
      counterintuitive to its main purpose, further harming the users' experiences (Valeriu, 2022).
    </p>
      
    <h2>Hate Speech</h2>
      
<div class="text-image-row">
        <div class="text-left">
          <p>
            X makes use of both employee moderators and content algorithms to detect and
            filter out harmful content from the rest, however, this system is far from 
            being perfect. A recent study has shown that X has been insufficiently dealing
            with hate speech, as out of 300 reported posts of antisemitism, only 34% of
            these had been properly moderated (CCDH, 2023). Racism, even online, is
            more than capable of affecting peoples' mental health (Tao &#38 Fisher, 2022), 
            the inability to efficiently remove such content undermines the platform's
            credibility and creates a hostile environment for many users. Online hate
            has been a constant problem affecting social media websites, and X's content
            moderation has currently been proven to be not enough to deal with this issue (Arun et al., 2024),
            which has caused a lot of problems for the platform and must be acted on in order
            to comply with their responsibility of providing a safe space.
          </p>
        </div>

        <div class="figures">
          <br />
          <img
            src="https://cdn.glitch.global/5556ebdb-685a-4d04-aa89-e5756f7b1ce6/elonstatement.png?v=1748236338540"
            alt="Musk saying hate speech will be removed"
          />
<p>
  <a href="https://www.techdirt.com/2023/01/31/as-expected-twitters-new-trust-safety-rules-are-elons-whims/" target="_blank">
    Figure 2. Elon Musk's Statement About Freedom of Speech via Masnick, 2023
  </a>
</p>
        </div>
      </div>

      <h2>Infringement on general Freedom of Speech</h2>
      <p>
        In a 'Tweet' on X, Elon Musk publicly spoke about his commitment in adhering to the
        US Government's standards of free speech (Musk, 2022), yet has taken many
        actions that go contrary to his stated commitments. For instance, Musk chose to ban
        multiple journalists that were frequently covering him (CNN, 2022). 
        Additionally, X's algorithmic moderation system has also inappropriately flagged and
        taken down posts that were not in violation of the platform's guidelines, which is
        counterintuitive to its main purpose and calls into question the reliability of its
        automated mechanisms in the role of moderation. Complicating the matter further is
        the international nature of the platform. The 'laws of free speech' aren't a hard and
        fast rule, as the standard for certain types of speech like hate speech or some 
        defamation can be different depending on the country, which places strain on enforcing
        these standards even harder for moderators or processing systems (Valeriu, 2022). A particularly unique example
        is during the starting years of Russia's invasion of Ukraine, where multiple freshly-created accounts were 
        suspended after a few days (Pierri at al., 2023). Navigating a 
        patchwork of legal and cultural standards without compromising its global user base 
        proves to have difficulties. 
      </p>

      <h2>Targeted Media</h2>

      <div class="text-image-row">
        <div class="text-left">
          <p>
            Leaked information from the "Twitter Files" — select internal documents — as
            well as X itself has revealed that X's internal algorithm is highly biased.
            Disproportionately amplifiying right-wing political content and associated 
            media outlets to users (Milmo, 2021). Something inherently unfair. This
            trend raises ethical concerns about the platform’s neutrality. Though this is a
            relatively mild case, it showcases how X can be used to shape users' opinions
            toward a specific group or party by favouring certain viewpoints and curated 
            exposure to specific content (Arun et al., 2024). This raises the broader issue of transparency and
            accountability: if a user's content feed is manipulated without their knowledge,
            then is the platform not facilitating free expression? In this scenario, another 
            user's content does not have an equal chance at being on their feed. The potential
            for abuse here is quite large, and also shows how much influence social media can
            have on shifting public ideas and ways of thinking, such as the inclusion of 
            'Twitter Blue' and payment for verification shaking public opinion in the
            legitimacy of popular public figures (Timm, 2024). 
          </p>
        </div>

        <div class="figures">
          <br />
          <img src="https://cdn.glitch.global/5556ebdb-685a-4d04-aa89-e5756f7b1ce6/trust.png?v=1748236344358"
            alt="Republican trust in X rises"/>
            <p>
              <a href="https://www.buzzfeednews.com/article/katienotopoulos/twitter-users-down-democrats-elon-musk" target="_blank">
                Figure 3. User Trust in Twitter/X via Notopoulos, 2023
              </a>
            </p>
        </div>
      </div>
        
    
       <hr>   
      <address>
        Made May 2025<br />
        by Group 4: Sameer Farhan, Kevin Sabdao, Archy Prajapati.
      </address>
    </div>
  </body>
</html>
