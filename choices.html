<!DOCTYPE html>
<html>
<head>
  <title>Choices</title>
  	<link rel="stylesheet" href="minimal-table.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
  </head>

<body>
<div id="circle"></div>
<ul class="navbar">
  <li><a href="index.html">Home</a></li>
  <li><a href="topic.html">Topic</a></li>
	<li><a href="opportunities.html">Opportunities</a></li>
	<li><a href="risks.html">Risks</a></li>
	<li><a href="choices.html">Choices</a></li>
	<!--<li><a href="ethics.html">Ethical Reflections</a></li>-->
	<li><a href="references.html">References</a></li>
   <li><a href="process.html">Process Support</a></li>
</ul>


<!-- Main content -->
    <div class="content">
      <div class="group"><p>G:4</p></div>
<h1>Technology Choices</h1>
<hr>
      <h2>Challenges</h2>
<p>
  Investigating the inconsistency of Twitter's (now X's) content moderation
  is a fairly specific topic, but it has opened several pathways and choices
  for us to conduct a deeper investigation. We have identified a number of
  focus areas for our inquiry which are based on X's choices and the impact
  of those choices:
  <ul>
    <li><strong>The perceived bias</strong> cultivated through inconsistent
      moderation influences user trust and public discourse (Valeriu, 2022). When users sense
      that content is being censored or promoted unevenly, it can damage the
      credibility of the platform and alter how conversations take place.</li>
  </ul>
      
  <ul>
    <li><strong>Reactions of users</strong> who feel that the platform fails
      to properly enforce their own rules fairly. Individuals may experience
      frustration when moderation seems selective, leading to the perception
      that certain voices are privileged over others — sometimes even depending
      on political alignment or status (Valeriu, 2022). This creates the potential to sway users
      into thinking a certain way for better engagement on the platform.</li>
  </ul>
      
  <ul>
    <li><strong>The broader impact on public debate</strong>, especially with
      political content or activism. Platforms like X serve as arenas for modern
      political dialogue and, similar to the point above, if moderation suppresses
      one side more than another, it can distort political discussions (Pierri et al., 2023). On a
      broader scale, this can lead to real-world shifts in thinking and activism
      which are heavily based on the extremist posts on the platform.</li>
    
  </ul>

  Furthermore, we have managed to gather multiple sources from news reports, first-person
  sources, and academic studies that have enabled our research, all of which can be
  found in our references. 

</p>
   
  <!--you have a choice to take either the red pill or the blue pill.-->
  
<h2>Choices</h2>
<p>
  However, beyond just analyzing X's issue, we must also ask: <i>What can individuals
  or collectives do in response to X’s biased moderation practices?</i>
  
  
  <ul><li><strong>Choosing Not to Participate</strong></li></ul>
  
  One option is to disengage from the platform entirely. Choosing not to participate
  in the use of X could act like a form of protest, signaling user-disapproval of X's
  moderation policies, while simultaneously <em>protecting</em> one's own freedom of
  expression. This solution, however, comes with its own drawbacks. Leaving the platform
  may mean losing access to necessary networks, and migrating to alternative social
  media platforms can introduce new issues — those platforms may also have <i>their own
  </i> biased moderation systems, leading to a cycle and a larger question being posed
  around social media (Klonick, 2019).

  
  <ul><li><strong>Demand Transparency and Accountability</strong></li></ul>
  
  Users can collectively demand greater transparency. This could involve demanding access
  to view the internal decision-making behind content regulation, demanding things such as
  the publication of moderation reports, clearer terms of service, or even the full release
  of the so-called "Twitter Files." (Valeriu, 2022). Even <i>public</i> apologies and help towards those who
  have been wronged by their system. This could all potentially help the transparency and
  credibility of X, possibly increasing user-trust over time. Peaceful protest, public
  petitions, and media pressure are all viable methods for driving change and encouraging X
  to be held accountable.
  
  <ul><li><strong>Encourage X to Provide Notes on Moderation Processes</strong></li></ul>
  
  Similar to the last point made; advocating for documented transparency that shows revision.
  If X were to provide an ongoing log or summary of changes to its moderation processes — such
  as updates to keyword filters or shifts in enforcement priorities — it would offer insight
  into how decisions are made and reduce the likelihood of biased content suppression (Pierri et al., 2023). This
  would not only empower users with knowledge but also enable X to reduce the need for constant
  moderation as users will know <i>exactly</i> what they can and cannot post. Additionally, X
  could employ external auditing bodies or ethics councils to oversee fairness (Klonick, 2019) — this is covered
  in our opportunities research.
  
  <br/><br/>
  These choices reflect the broader ethical dilemmas that emerge when X fails to uphold
  consistent standards of free expression. By taking action — whether through protest,
  advocacy, or disengagement — we, as users, can play an active role in shaping the
  platforms that influence our speech, identity, and public discourse. Whether these
  means are necessary actions to take, is up to users and will result from any actions
  X will take in the future.


</p>

  <hr> 
      <address>
        Made May 2025<br/>
        by Group 4: Sameer Farhan, Kevin Sabdao, Archy Prajapati.
      </address>
  </div>
  </body>
</html>