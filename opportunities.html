<!DOCTYPE html>
<html>
  <head>
    <title>Opportunities</title>
    <link rel="stylesheet" href="minimal-table.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
  </head>

  <body>
<div id="circle"></div>
    <!-- Site navigation menu -->
    <ul class="navbar">
  <li><a href="index.html">Home</a></li>
  <li><a href="topic.html">Topic</a></li>
	<li><a href="opportunities.html">Opportunities</a></li>
	<li><a href="risks.html">Risks</a></li>
	<li><a href="choices.html">Choices</a></li>
	<!--<li><a href="ethics.html">Ethical Reflections</a></li>-->
	<li><a href="references.html">References</a></li>
      <li><a href="process.html">Process Support</a></li>
</ul>


    <!-- Main content -->
        <div class="content">
      <div class="group"><p>G:4</p></div>
    <h1>Technology Opportunities</h1>
<hr>
      <p>
        Elon Musk once had the opportunity to reinforce Twitter's (now X’s) commitment
        to free expression. In a moment of public backlash over banning links to
        'competing' platforms, Musk tweeted and vowed, <em>“Going forward, there will
        be a vote for major policy changes. My apologies. Won’t happen again.” </em>
        (CNN, 2023). However, this promise was not upheld. Policy changes were
        implemented without meaningful user input, resulting in harmful consequences
        on all users across the platform. Journalists, in particular, were
        disproportionately targeted by these changes. This led to many journalists
        fleeing the platform to alternative platforms which were more supportive of the
        freedom of expression needed for working in the press. Ultimately, Musk fostered
        distrust and outrage, with X seeing a drop in usage from journalists, having a
        decline from 58% to 48% from 2023 to 2024 (Nair, 2025).

        If X were to seize the opportunity, it could rebuild trust by reforming its
        moderation framework—removing vague rules and adopting more transparent mechanisms
        similar to those seen on Facebook (Klonick, 2020). Furthermore, it could improve
        their moderation transparency through sharing less secretive <em>“Twitter Files”</em>
        rather than Musk’s carefully curated ‘leaks’ to falsify the sense of user-trust (Bond, 2022).
    </p>    
    
    <p>
          X, if it took the chance, could progress its app and meet the requirement
          of <em>protecting</em> free speech through many other opportunities.
          <br/><br/>
          We have brainstormed some potential routes that we believe could make the X's
          content moderation approach become more <em>reliable and lawful</em>, ultimately
          supporting the principle of free speech:

          <ul><li><strong>Context-based Moderation System</strong></li></ul>

          One promising opportunity lies in the development of context-aware moderation, 
          using both AI and human insight during moderation. Rather than flagging every
          post based solely on keywords or language models, the system can gather context
          and use it to justify the <em>removal</em>, hiding, or keeping of information (Arun et al., 2024). 
          For example, if a journalist is flagged discussing topics of public interest, 
          they should not be taken as a personal attack or hate speech which should take 
          more precedence. By concluding the <em>intent</em> through contextual understanding,
          X can avoid the wrongful punishing of valuable speech and minimizing the overstepping
          of power.

          <ul><li><strong>AI and Human Co-Moderation</strong></li></ul>

          Another effective model involves full-fledged AI-human collaboration (Valeriu, 2022). After AI detects
          a post that is <em>potentially</em> harmful, human moderators can step in and evaluate
          the content's meaning, tone, and possible impact on readers or viewers, deeming it
          non-malicious. This duo-system allows for the speed and scale of AI to be incorporated
          alongside the stronger reasoning, cultural awareness, and morals of humans (Valeriu, 2022). This allows
          for speed and accuracy, reduced removals of valid posts, increased removal of truly
          unethical posts, and provides a better safety net for users.
        

          <ul><li><strong>Promoting and Rewarding Constructive Speech</strong></li></ul>

          Rather than <i>only</i> focusing on removing harmful content, X could adopt recognising
          and rewarding constructive contributions as an approach to promote creating good content (CCDH, 2023).
          Continuous positive online behaviour — such as respectful discourse, informative posts,
          or community-building efforts — could be highlighted or incentivised through 'blue-tick'
          verification, increased visibility, or other forms of recognition. Leading by example
          could lead to increased conformity and hence, less need for moderation that could lead to
          bias. Even if it seems like another 'trend' for users to follow, a positive outcome is
          what X can expect to gain — users tend to follow a path where they can gain praise and
          therefore gain more fame or influence.
          

          <ul><li><strong>User-Controlled Moderation Tools</strong></li></ul>

          Empowering users to customise their experience is another path forward. X could introduce
          filters and controls that allow individuals to tailor the content they see, giving users
          more choice in the curation of their feeds (Pierri et al., 2023). However, this would be experimental as it <em>
          could</em> lead to a disunited platform by making niches that rarely interact with each other.
          But through a thoughtful design, the implementation of a processing system specific towards
          individuals could mean that X can rely more on <em>users' choices</em>, rather than the current
          biased content moderation system. It would shift the power dynamic from platform to user, 
          making X more user-driven and less prone to uproar over X being responsible for the censorship
          issue.
          
          <br/><br/>
          In summary, these opportunities highlight that moderation does not have to come at
          the expense of free speech. By implementing transparent systems, X could re-establish
          itself as a platform — one that balances safety with freedom, and regulation with fairness.
    </p>

    
    <!--citations cleared up now -->
<hr>

    
      <address>
        Made May 2025<br />
        by Group 4: Sameer Farhan, Kevin Sabdao, Archy Prajapati.
      </address>
    <!-- Sign and date the page, it's only polite! -->    </div>
  </body>
</html>
